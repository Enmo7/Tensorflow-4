{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align=\"center\"><font size=\"5\">LOGISTIC REGRESSION WITH TENSORFLOW</font></h1>\n"
      ],
      "metadata": {
        "id": "fuQPjOzUG0T4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z7xcCjsYGR_S"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "iris_X, iris_y = iris.data[:-1,:], iris.target[:-1]\n",
        "iris_y= pd.get_dummies(iris_y).values\n",
        "trainX, testX, trainY, testY = train_test_split(iris_X, iris_y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "BwZCyuBPGu42"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# numFeatures is the number of features in our input data.\n",
        "# In the iris dataset, this number is '4'.\n",
        "numFeatures = trainX.shape[1]\n",
        "print('numFeatures is : ', numFeatures )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRIEghBFHCO6",
        "outputId": "ec94ca96-a83a-4454-8a2f-b15d674ddd6c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numFeatures is :  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# numLabels is the number of classes our data points can be in.\n",
        "# In the iris dataset, this number is '3'.\n",
        "numLabels = trainY.shape[1]\n",
        "print('numLabels is : ', numLabels )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JlbARD6HI_Y",
        "outputId": "da135c15-987a-4863-8d2e-7d5918739172"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numLabels is :  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.Variable( np.identity(numFeatures), tf.TensorShape(numFeatures),dtype='float32') # Iris has 4 features, so X is a tensor to hold our data.\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpqttT-3HI2k",
        "outputId": "39c8c6db-0fe5-403e-9abe-2f46543b213b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(4, 4) dtype=float32, numpy=\n",
              "array([[1., 0., 0., 0.],\n",
              "       [0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0.],\n",
              "       [0., 0., 0., 1.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yGold = tf.Variable(np.array([1,1,1]),shape=tf.TensorShape(numLabels),dtype='float32') # This will be our correct answers matrix for 3 classes.\n",
        "yGold"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBE-GB8rHYBf",
        "outputId": "beb4074a-76a3-4c91-a6ed-f0b8bd4ef30c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([1., 1., 1.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainX = tf.constant(trainX, dtype='float32')\n",
        "trainX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6bDMwoaHblz",
        "outputId": "abfeb802-cf47-46f3-bf9c-5a43e1830e4d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(99, 4), dtype=float32, numpy=\n",
              "array([[5.7, 3. , 4.2, 1.2],\n",
              "       [6.5, 3. , 5.8, 2.2],\n",
              "       [5.6, 3. , 4.5, 1.5],\n",
              "       [5.1, 3.5, 1.4, 0.2],\n",
              "       [6.9, 3.1, 5.1, 2.3],\n",
              "       [5.8, 2.7, 4.1, 1. ],\n",
              "       [5.2, 3.4, 1.4, 0.2],\n",
              "       [5. , 3.5, 1.3, 0.3],\n",
              "       [5.1, 3.8, 1.9, 0.4],\n",
              "       [5. , 2. , 3.5, 1. ],\n",
              "       [7.7, 2.8, 6.7, 2. ],\n",
              "       [4.8, 3.4, 1.9, 0.2],\n",
              "       [5. , 3. , 1.6, 0.2],\n",
              "       [5.1, 3.3, 1.7, 0.5],\n",
              "       [5. , 2.3, 3.3, 1. ],\n",
              "       [5.1, 3.4, 1.5, 0.2],\n",
              "       [5.6, 2.7, 4.2, 1.3],\n",
              "       [6. , 3. , 4.8, 1.8],\n",
              "       [4.6, 3.2, 1.4, 0.2],\n",
              "       [5.7, 2.9, 4.2, 1.3],\n",
              "       [6.8, 3. , 5.5, 2.1],\n",
              "       [5.5, 4.2, 1.4, 0.2],\n",
              "       [5.1, 2.5, 3. , 1.1],\n",
              "       [6.7, 3. , 5.2, 2.3],\n",
              "       [6. , 2.2, 4. , 1. ],\n",
              "       [5.4, 3. , 4.5, 1.5],\n",
              "       [6. , 2.2, 5. , 1.5],\n",
              "       [5.5, 2.3, 4. , 1.3],\n",
              "       [5.4, 3.9, 1.7, 0.4],\n",
              "       [6.5, 3. , 5.2, 2. ],\n",
              "       [6.4, 3.1, 5.5, 1.8],\n",
              "       [5. , 3.3, 1.4, 0.2],\n",
              "       [5. , 3.2, 1.2, 0.2],\n",
              "       [5.5, 2.4, 3.8, 1.1],\n",
              "       [6.7, 3. , 5. , 1.7],\n",
              "       [4.9, 3.1, 1.5, 0.2],\n",
              "       [5.7, 2.5, 5. , 2. ],\n",
              "       [5. , 3.4, 1.5, 0.2],\n",
              "       [5. , 3.5, 1.6, 0.6],\n",
              "       [5.9, 3.2, 4.8, 1.8],\n",
              "       [6.2, 2.9, 4.3, 1.3],\n",
              "       [6. , 2.7, 5.1, 1.6],\n",
              "       [6.3, 2.8, 5.1, 1.5],\n",
              "       [6.1, 2.6, 5.6, 1.4],\n",
              "       [5.5, 2.5, 4. , 1.3],\n",
              "       [4.4, 2.9, 1.4, 0.2],\n",
              "       [4.3, 3. , 1.1, 0.1],\n",
              "       [7.7, 2.6, 6.9, 2.3],\n",
              "       [6.2, 2.8, 4.8, 1.8],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5.1, 3.5, 1.4, 0.3],\n",
              "       [4.4, 3. , 1.3, 0.2],\n",
              "       [6.3, 2.5, 4.9, 1.5],\n",
              "       [7.7, 3. , 6.1, 2.3],\n",
              "       [4.6, 3.4, 1.4, 0.3],\n",
              "       [6.4, 2.7, 5.3, 1.9],\n",
              "       [6.3, 3.3, 6. , 2.5],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [6.1, 2.9, 4.7, 1.4],\n",
              "       [6.5, 2.8, 4.6, 1.5],\n",
              "       [7.2, 3.2, 6. , 1.8],\n",
              "       [7. , 3.2, 4.7, 1.4],\n",
              "       [5.8, 2.8, 5.1, 2.4],\n",
              "       [5.1, 3.8, 1.6, 0.2],\n",
              "       [6.4, 3.2, 5.3, 2.3],\n",
              "       [5.9, 3. , 4.2, 1.5],\n",
              "       [6.3, 2.5, 5. , 1.9],\n",
              "       [5.7, 2.6, 3.5, 1. ],\n",
              "       [5.2, 2.7, 3.9, 1.4],\n",
              "       [6.1, 3. , 4.6, 1.4],\n",
              "       [4.5, 2.3, 1.3, 0.3],\n",
              "       [6.6, 2.9, 4.6, 1.3],\n",
              "       [5.5, 2.6, 4.4, 1.2],\n",
              "       [5.3, 3.7, 1.5, 0.2],\n",
              "       [5.6, 3. , 4.1, 1.3],\n",
              "       [7.3, 2.9, 6.3, 1.8],\n",
              "       [6.7, 3.3, 5.7, 2.1],\n",
              "       [5.1, 3.7, 1.5, 0.4],\n",
              "       [4.9, 2.4, 3.3, 1. ],\n",
              "       [6.8, 3.2, 5.9, 2.3],\n",
              "       [7.2, 3. , 5.8, 1.6],\n",
              "       [4.9, 3.6, 1.4, 0.1],\n",
              "       [6.9, 3.1, 5.4, 2.1],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [6.9, 3.1, 4.9, 1.5],\n",
              "       [7.4, 2.8, 6.1, 1.9],\n",
              "       [6.3, 2.9, 5.6, 1.8],\n",
              "       [5.7, 2.8, 4.1, 1.3],\n",
              "       [6.5, 3. , 5.5, 1.8],\n",
              "       [6.3, 2.3, 4.4, 1.3],\n",
              "       [6.4, 2.9, 4.3, 1.3],\n",
              "       [5.6, 2.8, 4.9, 2. ],\n",
              "       [6.2, 3.4, 5.4, 2.3],\n",
              "       [5.4, 3.4, 1.7, 0.2],\n",
              "       [6.1, 2.8, 4. , 1.3],\n",
              "       [4.9, 2.5, 4.5, 1.7],\n",
              "       [5.8, 4. , 1.2, 0.2],\n",
              "       [5.8, 2.6, 4. , 1.2],\n",
              "       [7.1, 3. , 5.9, 2.1]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainY = tf.constant(trainY, dtype='float32')\n",
        "trainY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNAw80g2Hg78",
        "outputId": "35c58518-9715-495d-f635-8f4121f4c8bd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(99, 3), dtype=float32, numpy=\n",
              "array([[0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testX = tf.constant(testX, dtype='float32')\n",
        "testX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRWSzum9Hgyc",
        "outputId": "11e52504-7681-4cc9-88dd-5757e1dea9af"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(50, 4), dtype=float32, numpy=\n",
              "array([[6.1, 2.8, 4.7, 1.2],\n",
              "       [5.7, 3.8, 1.7, 0.3],\n",
              "       [7.7, 3.8, 6.7, 2.2],\n",
              "       [6. , 2.9, 4.5, 1.5],\n",
              "       [6.8, 2.8, 4.8, 1.4],\n",
              "       [5.4, 3.4, 1.5, 0.4],\n",
              "       [5.6, 2.9, 3.6, 1.3],\n",
              "       [6.7, 3.1, 5.6, 2.4],\n",
              "       [6.2, 2.2, 4.5, 1.5],\n",
              "       [5.8, 2.7, 3.9, 1.2],\n",
              "       [7.2, 3.6, 6.1, 2.5],\n",
              "       [4.8, 3. , 1.4, 0.1],\n",
              "       [5.5, 3.5, 1.3, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.1],\n",
              "       [5.1, 3.8, 1.5, 0.3],\n",
              "       [6.3, 3.3, 4.7, 1.6],\n",
              "       [6.3, 3.4, 5.6, 2.4],\n",
              "       [5.6, 2.5, 3.9, 1.1],\n",
              "       [5.7, 2.8, 4.5, 1.3],\n",
              "       [7.9, 3.8, 6.4, 2. ],\n",
              "       [4.7, 3.2, 1.6, 0.2],\n",
              "       [6.3, 2.7, 4.9, 1.8],\n",
              "       [5. , 3.4, 1.6, 0.4],\n",
              "       [6.1, 3. , 4.9, 1.8],\n",
              "       [6.4, 2.8, 5.6, 2.1],\n",
              "       [6.7, 3.3, 5.7, 2.5],\n",
              "       [6.5, 3.2, 5.1, 2. ],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [4.8, 3. , 1.4, 0.3],\n",
              "       [4.8, 3.1, 1.6, 0.2],\n",
              "       [4.6, 3.6, 1. , 0.2],\n",
              "       [5.7, 4.4, 1.5, 0.4],\n",
              "       [6.7, 3.1, 4.4, 1.4],\n",
              "       [4.8, 3.4, 1.6, 0.2],\n",
              "       [4.4, 3.2, 1.3, 0.2],\n",
              "       [7.6, 3. , 6.6, 2.1],\n",
              "       [6.4, 3.2, 4.5, 1.5],\n",
              "       [5.2, 3.5, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2],\n",
              "       [5.2, 4.1, 1.5, 0.1],\n",
              "       [6.9, 3.2, 5.7, 2.3],\n",
              "       [6. , 3.4, 4.5, 1.6],\n",
              "       [6.7, 3.1, 4.7, 1.5],\n",
              "       [5.4, 3.9, 1.3, 0.4],\n",
              "       [5.4, 3.7, 1.5, 0.2],\n",
              "       [5.5, 2.4, 3.7, 1. ],\n",
              "       [6.4, 2.8, 5.6, 2.2],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [6.6, 3. , 4.4, 1.4],\n",
              "       [6.7, 2.5, 5.8, 1.8]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testY = tf.constant(testY, dtype='float32')\n",
        "testY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbBk-5cjHgs4",
        "outputId": "0e0e1f89-5a5b-441c-fff1-2dd8261047e3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(50, 3), dtype=float32, numpy=\n",
              "array([[0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W = tf.Variable(tf.zeros([4, 3]))  # 4-dimensional input and  3 classes\n",
        "W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpY5cpNuHo4F",
        "outputId": "c485c864-271e-44ee-e3c0-190ea312f809"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(4, 3) dtype=float32, numpy=\n",
              "array([[0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.],\n",
              "       [0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = tf.Variable(tf.zeros([3])) # 3-dimensional output [0,0,1],[0,1,0],[1,0,0]\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSxy-gCOHuEn",
        "outputId": "30318df3-58d0-445b-896b-5d33dee699d1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Randomly sample from a normal distribution with standard deviation .01\n",
        "\n",
        "weights = tf.Variable(tf.random.normal([numFeatures,numLabels],\n",
        "                                       mean=0.,\n",
        "                                       stddev=0.01,\n",
        "                                       name=\"weights\"),dtype='float32')"
      ],
      "metadata": {
        "id": "abV4Rm1zHyBl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPJaXHn1H2zO",
        "outputId": "2e765af2-2632-4000-bf98-245a3a5b9743"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(4, 3) dtype=float32, numpy=\n",
              "array([[-0.00104856,  0.00147907, -0.00505806],\n",
              "       [-0.00702136, -0.008312  , -0.00100379],\n",
              "       [-0.01567086,  0.00174015, -0.00472187],\n",
              "       [ 0.01170275,  0.00880664,  0.00996475]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bias = tf.Variable(tf.random.normal([1,numLabels],\n",
        "                                    mean=0.,\n",
        "                                    stddev=0.01,\n",
        "                                    name=\"bias\"))"
      ],
      "metadata": {
        "id": "zIg9l-sFH49e"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rrysh_IpH8U8",
        "outputId": "81e25e5c-fe20-4a9c-f7b3-ab41b37780f4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(1, 3) dtype=float32, numpy=array([[0.02112216, 0.00500387, 0.00278158]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression model"
      ],
      "metadata": {
        "id": "_IE-5ScPIEZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Three-component breakdown of the Logistic Regression equation.\n",
        "# Note that these feed into each other.\n",
        "def logistic_regression(x):\n",
        "    apply_weights_OP = tf.matmul(x, weights, name=\"apply_weights\")\n",
        "    add_bias_OP = tf.add(apply_weights_OP, bias, name=\"add_bias\") \n",
        "    activation_OP = tf.nn.sigmoid(add_bias_OP, name=\"activation\")\n",
        "    return activation_OP"
      ],
      "metadata": {
        "id": "Qqf3yoNQH96E"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of Epochs in our training\n",
        "numEpochs = 700\n",
        "numEpochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vfw3cb1IPt5",
        "outputId": "351bcf31-2b04-4c3f-967b-b68a8d216f12"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "700"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining our learning rate iterations (decay)\n",
        "learningRate = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0008,\n",
        "                                          decay_steps=trainX.shape[0],\n",
        "                                          decay_rate= 0.95,\n",
        "                                          staircase=True)"
      ],
      "metadata": {
        "id": "rghOSi07IW3w"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learningRate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rlq4nCPtIZ68",
        "outputId": "83d0b082-e5ca-4efe-aa45-7c20cb6517f9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.optimizers.schedules.learning_rate_schedule.ExponentialDecay at 0x7f208c3ca3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining our cost function - Squared Mean Error\n",
        "loss_object = tf.keras.losses.MeanSquaredLogarithmicError()\n",
        "loss_object"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU8cjHlGIaa_",
        "outputId": "a03a215a-78bc-497e-f07a-1c4916d50072"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.losses.MeanSquaredLogarithmicError at 0x7f208c3cab50>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.SGD(learningRate)\n",
        "optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx2xuE72IgBl",
        "outputId": "c5cb0772-9d7f-45bc-c32f-10e9dbd55401"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.optimizers.optimizer_experimental.sgd.SGD at 0x7f208c3ca0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy metric.\n",
        "def accuracy(y_pred, y_true):\n",
        "# Predicted class is the index of the highest score in prediction vector (i.e. argmax).\n",
        "\n",
        "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
        "\n",
        "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "metadata": {
        "id": "EGyTAmN_Iinb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimization process. \n",
        "\n",
        "def run_optimization(x, y):\n",
        "    with tf.GradientTape() as g:\n",
        "        pred = logistic_regression(x)\n",
        "        loss = loss_object(pred, y)\n",
        "    gradients = g.gradient(loss, [weights, bias])\n",
        "    optimizer.apply_gradients(zip(gradients, [weights, bias]))"
      ],
      "metadata": {
        "id": "2DftbivDIlQH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize reporting variables\n",
        "display_step = 10\n",
        "epoch_values = []\n",
        "accuracy_values = []\n",
        "loss_values = []\n",
        "loss = 0\n",
        "diff = 1\n",
        "# Training epochs\n",
        "for i in range(numEpochs):\n",
        "    if i > 1 and diff < .0001:\n",
        "        print(\"change in loss %g; convergence.\"%diff)\n",
        "        break\n",
        "    else:\n",
        "        # Run training step\n",
        "        run_optimization(trainX, trainY)\n",
        "        \n",
        "        # Report occasional stats\n",
        "        if i % display_step == 0:\n",
        "            # Add epoch to epoch_values\n",
        "            epoch_values.append(i)\n",
        "            \n",
        "            pred = logistic_regression(testX)\n",
        "\n",
        "            newLoss = loss_object(pred, testY)\n",
        "            # Add loss to live graphing variable\n",
        "            loss_values.append(newLoss)\n",
        "            \n",
        "            # Generate accuracy stats on test data\n",
        "            acc = accuracy(pred, testY)\n",
        "            accuracy_values.append(acc)\n",
        "            \n",
        "    \n",
        "            # Re-assign values for variables\n",
        "            diff = abs(newLoss - loss)\n",
        "            loss = newLoss\n",
        "\n",
        "            #generate print statements\n",
        "            print(\"step %d, training accuracy %g, loss %g, change in loss %g\"%(i, acc, newLoss, diff))\n",
        "\n",
        "        \n",
        "\n",
        "          \n",
        "\n",
        "# How well do we perform on held-out test data?\n",
        "print(\"final accuracy on test set: %s\" %acc.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rytKBVtCIoBI",
        "outputId": "639e1bf6-59b8-4871-c6e6-f58df48aa85c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, training accuracy 0.3, loss 0.13502, change in loss 0.13502\n",
            "step 10, training accuracy 0.3, loss 0.134452, change in loss 0.000567928\n",
            "step 20, training accuracy 0.3, loss 0.133893, change in loss 0.000559613\n",
            "step 30, training accuracy 0.3, loss 0.133342, change in loss 0.000551313\n",
            "step 40, training accuracy 0.3, loss 0.132798, change in loss 0.000543028\n",
            "step 50, training accuracy 0.3, loss 0.132264, change in loss 0.000534758\n",
            "step 60, training accuracy 0.3, loss 0.131737, change in loss 0.000526533\n",
            "step 70, training accuracy 0.3, loss 0.131219, change in loss 0.000518292\n",
            "step 80, training accuracy 0.3, loss 0.130709, change in loss 0.000510141\n",
            "step 90, training accuracy 0.3, loss 0.130207, change in loss 0.00050199\n",
            "step 100, training accuracy 0.3, loss 0.129718, change in loss 0.000488997\n",
            "step 110, training accuracy 0.3, loss 0.129256, change in loss 0.000461861\n",
            "step 120, training accuracy 0.3, loss 0.128801, change in loss 0.00045462\n",
            "step 130, training accuracy 0.3, loss 0.128354, change in loss 0.000447497\n",
            "step 140, training accuracy 0.3, loss 0.127913, change in loss 0.000440374\n",
            "step 150, training accuracy 0.3, loss 0.12748, change in loss 0.000433356\n",
            "step 160, training accuracy 0.3, loss 0.127054, change in loss 0.000426367\n",
            "step 170, training accuracy 0.3, loss 0.126634, change in loss 0.000419453\n",
            "step 180, training accuracy 0.3, loss 0.126222, change in loss 0.000412628\n",
            "step 190, training accuracy 0.3, loss 0.125816, change in loss 0.000405803\n",
            "step 200, training accuracy 0.3, loss 0.125423, change in loss 0.000393182\n",
            "step 210, training accuracy 0.3, loss 0.12505, change in loss 0.00037311\n",
            "step 220, training accuracy 0.3, loss 0.124682, change in loss 0.000367187\n",
            "step 230, training accuracy 0.3, loss 0.124321, change in loss 0.000361308\n",
            "step 240, training accuracy 0.32, loss 0.123965, change in loss 0.000355542\n",
            "step 250, training accuracy 0.3, loss 0.123616, change in loss 0.000349797\n",
            "step 260, training accuracy 0.34, loss 0.123272, change in loss 0.000344142\n",
            "step 270, training accuracy 0.42, loss 0.122933, change in loss 0.000338539\n",
            "step 280, training accuracy 0.44, loss 0.1226, change in loss 0.000333048\n",
            "step 290, training accuracy 0.38, loss 0.122272, change in loss 0.000327595\n",
            "step 300, training accuracy 0.36, loss 0.121957, change in loss 0.000315838\n",
            "step 310, training accuracy 0.34, loss 0.121655, change in loss 0.000301301\n",
            "step 320, training accuracy 0.32, loss 0.121359, change in loss 0.000296585\n",
            "step 330, training accuracy 0.32, loss 0.121067, change in loss 0.000291921\n",
            "step 340, training accuracy 0.32, loss 0.120779, change in loss 0.000287332\n",
            "step 350, training accuracy 0.32, loss 0.120497, change in loss 0.000282802\n",
            "step 360, training accuracy 0.32, loss 0.120218, change in loss 0.000278331\n",
            "step 370, training accuracy 0.32, loss 0.119944, change in loss 0.000273921\n",
            "step 380, training accuracy 0.32, loss 0.119675, change in loss 0.000269577\n",
            "step 390, training accuracy 0.32, loss 0.119409, change in loss 0.000265315\n",
            "step 400, training accuracy 0.32, loss 0.119155, change in loss 0.000254609\n",
            "step 410, training accuracy 0.32, loss 0.118911, change in loss 0.000244297\n",
            "step 420, training accuracy 0.32, loss 0.11867, change in loss 0.000240587\n",
            "step 430, training accuracy 0.32, loss 0.118433, change in loss 0.000236966\n",
            "step 440, training accuracy 0.32, loss 0.1182, change in loss 0.000233352\n",
            "step 450, training accuracy 0.32, loss 0.11797, change in loss 0.000229836\n",
            "step 460, training accuracy 0.32, loss 0.117743, change in loss 0.000226364\n",
            "step 470, training accuracy 0.32, loss 0.117521, change in loss 0.000222914\n",
            "step 480, training accuracy 0.32, loss 0.117301, change in loss 0.000219554\n",
            "step 490, training accuracy 0.32, loss 0.117085, change in loss 0.000216216\n",
            "step 500, training accuracy 0.32, loss 0.116878, change in loss 0.000206612\n",
            "step 510, training accuracy 0.32, loss 0.116679, change in loss 0.000199392\n",
            "step 520, training accuracy 0.32, loss 0.116482, change in loss 0.000196539\n",
            "step 530, training accuracy 0.32, loss 0.116288, change in loss 0.000193708\n",
            "step 540, training accuracy 0.32, loss 0.116098, change in loss 0.000190936\n",
            "step 550, training accuracy 0.32, loss 0.115909, change in loss 0.000188194\n",
            "step 560, training accuracy 0.32, loss 0.115724, change in loss 0.000185505\n",
            "step 570, training accuracy 0.32, loss 0.115541, change in loss 0.000182837\n",
            "step 580, training accuracy 0.32, loss 0.115361, change in loss 0.000180222\n",
            "step 590, training accuracy 0.32, loss 0.115183, change in loss 0.000177659\n",
            "step 600, training accuracy 0.32, loss 0.115014, change in loss 0.000169031\n",
            "step 610, training accuracy 0.32, loss 0.11485, change in loss 0.000164129\n",
            "step 620, training accuracy 0.32, loss 0.114688, change in loss 0.000161916\n",
            "step 630, training accuracy 0.32, loss 0.114528, change in loss 0.000159733\n",
            "step 640, training accuracy 0.32, loss 0.114371, change in loss 0.000157565\n",
            "step 650, training accuracy 0.32, loss 0.114215, change in loss 0.000155456\n",
            "step 660, training accuracy 0.32, loss 0.114062, change in loss 0.00015337\n",
            "step 670, training accuracy 0.32, loss 0.113911, change in loss 0.000151306\n",
            "step 680, training accuracy 0.32, loss 0.113761, change in loss 0.000149295\n",
            "step 690, training accuracy 0.32, loss 0.113614, change in loss 0.000147283\n",
            "final accuracy on test set: 0.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(loss_values)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "EhcHolCaIsgd",
        "outputId": "ea3fbd36-a118-4e38-e400-6f4b565c5a3e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}